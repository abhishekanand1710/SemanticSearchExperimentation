{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"As part of this notebook, I implement assymetric semantic search with the retrieve and re-rank pipeline - [Retrive-Rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html). The sentence encoding part is similar to all other notebooks, but we don't use FAISS here. We first fetch top k passages from our dataset for the query, with k being a number >= 100. Then we use Cross-Encoder to rank the top k responses.","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:00:01.449255Z","iopub.execute_input":"2023-05-05T19:00:01.449839Z","iopub.status.idle":"2023-05-05T19:00:15.508661Z","shell.execute_reply.started":"2023-05-05T19:00:01.449804Z","shell.execute_reply":"2023-05-05T19:00:15.507461Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (4.27.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (4.64.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.13.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.11.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.2)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.11.10)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence_transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence_transformers) (9.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.11.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=80ad1cac22e0934d8615a0198798e904185053fcb7d9ff1d3780f761587670b3\n  Stored in directory: /root/.cache/pip/wheels/83/71/2b/40d17d21937fed496fb99145227eca8f20b4891240ff60c86f\nSuccessfully built sentence_transformers\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom string import digits\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nimport re\nfrom tqdm import tqdm, notebook\n\nfrom sentence_transformers import SentenceTransformer, CrossEncoder, util\nimport torch\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-05T19:00:30.271905Z","iopub.execute_input":"2023-05-05T19:00:30.272297Z","iopub.status.idle":"2023-05-05T19:00:34.162055Z","shell.execute_reply.started":"2023-05-05T19:00:30.272261Z","shell.execute_reply":"2023-05-05T19:00:34.160935Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n/kaggle/input/arxiv-cs-papers-abstract-from-2010/cs_arxiv_from_2010.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"docs_df = pd.read_csv('/kaggle/input/arxiv-cs-papers-abstract-from-2010/cs_arxiv_from_2010.csv')\ndocs_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:00:36.202083Z","iopub.execute_input":"2023-05-05T19:00:36.203011Z","iopub.status.idle":"2023-05-05T19:00:50.998430Z","shell.execute_reply.started":"2023-05-05T19:00:36.202971Z","shell.execute_reply":"2023-05-05T19:00:50.997292Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         id                                            authors  \\\n0  704.0213              Ketan D. Mulmuley Hariharan Narayanan   \n1  704.1409                                      Yao HengShuai   \n2  704.1829  Stefan Felsner, Kamil Kloch, Grzegorz Matecki,...   \n3  705.0561                                     Jing-Chao Chen   \n4  705.1025                                     David Eppstein   \n\n                                               title            category  \\\n0  Geometric Complexity Theory V: On deciding non...           ['cs.CC']   \n1        Preconditioned Temporal Difference Learning  ['cs.LG', 'cs.AI']   \n2  On-line Chain Partitions of Up-growing Semi-or...           ['cs.DM']   \n3  Iterative Rounding for the Closest String Problem  ['cs.DS', 'cs.CC']   \n4        Recognizing Partial Cubes in Quadratic Time           ['cs.DS']   \n\n                                            abstract  \n0    This article has been withdrawn because it h...  \n1    This paper has been withdrawn by the author....  \n2    On-line chain partition is a two-player game...  \n3    The closest string problem is an NP-hard pro...  \n4    We show how to test whether a graph with n v...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>authors</th>\n      <th>title</th>\n      <th>category</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>704.0213</td>\n      <td>Ketan D. Mulmuley Hariharan Narayanan</td>\n      <td>Geometric Complexity Theory V: On deciding non...</td>\n      <td>['cs.CC']</td>\n      <td>This article has been withdrawn because it h...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>704.1409</td>\n      <td>Yao HengShuai</td>\n      <td>Preconditioned Temporal Difference Learning</td>\n      <td>['cs.LG', 'cs.AI']</td>\n      <td>This paper has been withdrawn by the author....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>704.1829</td>\n      <td>Stefan Felsner, Kamil Kloch, Grzegorz Matecki,...</td>\n      <td>On-line Chain Partitions of Up-growing Semi-or...</td>\n      <td>['cs.DM']</td>\n      <td>On-line chain partition is a two-player game...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>705.0561</td>\n      <td>Jing-Chao Chen</td>\n      <td>Iterative Rounding for the Closest String Problem</td>\n      <td>['cs.DS', 'cs.CC']</td>\n      <td>The closest string problem is an NP-hard pro...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>705.1025</td>\n      <td>David Eppstein</td>\n      <td>Recognizing Partial Cubes in Quadratic Time</td>\n      <td>['cs.DS']</td>\n      <td>We show how to test whether a graph with n v...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dim=384","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:00:51.000586Z","iopub.execute_input":"2023-05-05T19:00:51.001482Z","iopub.status.idle":"2023-05-05T19:00:51.009443Z","shell.execute_reply.started":"2023-05-05T19:00:51.001442Z","shell.execute_reply":"2023-05-05T19:00:51.007616Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\nif torch.cuda.is_available():      \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:00:51.010956Z","iopub.execute_input":"2023-05-05T19:00:51.012243Z","iopub.status.idle":"2023-05-05T19:00:51.108845Z","shell.execute_reply.started":"2023-05-05T19:00:51.012189Z","shell.execute_reply":"2023-05-05T19:00:51.107859Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"docs_text = (docs_df['title'] + ' ' + docs_df['abstract']).values.tolist()\ndocs_text[:5]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:00:51.114133Z","iopub.execute_input":"2023-05-05T19:00:51.116644Z","iopub.status.idle":"2023-05-05T19:00:51.934332Z","shell.execute_reply.started":"2023-05-05T19:00:51.116602Z","shell.execute_reply":"2023-05-05T19:00:51.933143Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[\"Geometric Complexity Theory V: On deciding nonvanishing of a generalized\\n  Littlewood-Richardson coefficient   This article has been withdrawn because it has been merged with the earlier\\narticle GCT3 (arXiv: CS/0501076 [cs.CC]) in the series. The merged article is\\nnow available as:\\n  Geometric Complexity Theory III: on deciding nonvanishing of a\\nLittlewood-Richardson Coefficient, Journal of Algebraic Combinatorics, vol. 36,\\nissue 1, 2012, pp. 103-110. (Authors: Ketan Mulmuley, Hari Narayanan and Milind\\nSohoni)\\n  The new article in this GCT5 slot in the series is:\\n  Geometric Complexity Theory V: Equivalence between blackbox derandomization\\nof polynomial identity testing and derandomization of Noether's Normalization\\nLemma, in the Proceedings of FOCS 2012 (abstract), arXiv:1209.5993 [cs.CC]\\n(full version) (Author: Ketan Mulmuley)\\n\",\n 'Preconditioned Temporal Difference Learning   This paper has been withdrawn by the author. This draft is withdrawn for its\\npoor quality in english, unfortunately produced by the author when he was just\\nstarting his science route. Look at the ICML version instead:\\nhttp://icml2008.cs.helsinki.fi/papers/111.pdf\\n',\n 'On-line Chain Partitions of Up-growing Semi-orders   On-line chain partition is a two-player game between Spoiler and Algorithm.\\nSpoiler presents a partially ordered set, point by point. Algorithm assigns\\nincoming points (immediately and irrevocably) to the chains which constitute a\\nchain partition of the order. The value of the game for orders of width $w$ is\\na minimum number $\\\\fVal(w)$ such that Algorithm has a strategy using at most\\n$\\\\fVal(w)$ chains on orders of width at most $w$. We analyze the chain\\npartition game for up-growing semi-orders. Surprisingly, the golden ratio comes\\ninto play and the value of the game is $\\\\lfloor\\\\frac{1+\\\\sqrt{5}}{2}\\\\; w\\n\\\\rfloor$.\\n',\n 'Iterative Rounding for the Closest String Problem   The closest string problem is an NP-hard problem, whose task is to find a\\nstring that minimizes maximum Hamming distance to a given set of strings. This\\ncan be reduced to an integer program (IP). However, to date, there exists no\\nknown polynomial-time algorithm for IP. In 2004, Meneses et al. introduced a\\nbranch-and-bound (B & B) method for solving the IP problem. Their algorithm is\\nnot always efficient and has the exponential time complexity. In the paper, we\\nattempt to solve efficiently the IP problem by a greedy iterative rounding\\ntechnique. The proposed algorithm is polynomial time and much faster than the\\nexisting B & B IP for the CSP. If the number of strings is limited to 3, the\\nalgorithm is provably at most 1 away from the optimum. The empirical results\\nshow that in many cases we can find an exact solution. Even though we fail to\\nfind an exact solution, the solution found is very close to exact solution.\\n',\n 'Recognizing Partial Cubes in Quadratic Time   We show how to test whether a graph with n vertices and m edges is a partial\\ncube, and if so how to find a distance-preserving embedding of the graph into a\\nhypercube, in the near-optimal time bound O(n^2), improving previous O(nm)-time\\nsolutions.\\n']"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(text, remove_stopwords=True):\n    text = text.lower()\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n    text = re.sub(r'<br />', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    \n    if remove_stopwords:\n        text = text.split()\n        stops = set(stopwords.words('english'))\n        text = [w for w in text if w not in stops]\n        text = ' '.join(text)\n        \n    return text","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:00:54.940292Z","iopub.execute_input":"2023-05-05T19:00:54.941008Z","iopub.status.idle":"2023-05-05T19:00:54.948570Z","shell.execute_reply.started":"2023-05-05T19:00:54.940970Z","shell.execute_reply":"2023-05-05T19:00:54.947391Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def clean_data(data):\n    cleaned_data = []\n    for doc in notebook.tqdm(data):\n        text = clean_text(doc, False)\n        cleaned_data.append(text)\n    return cleaned_data","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:00:56.226339Z","iopub.execute_input":"2023-05-05T19:00:56.226743Z","iopub.status.idle":"2023-05-05T19:00:56.233146Z","shell.execute_reply.started":"2023-05-05T19:00:56.226701Z","shell.execute_reply":"2023-05-05T19:00:56.231908Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"cleaned_docs = clean_data(docs_text)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:01:19.141909Z","iopub.execute_input":"2023-05-05T19:01:19.142345Z","iopub.status.idle":"2023-05-05T19:01:30.585792Z","shell.execute_reply.started":"2023-05-05T19:01:19.142302Z","shell.execute_reply":"2023-05-05T19:01:30.584672Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/484027 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cad0fe2d38748a99544d338ddc3397e"}},"metadata":{}}]},{"cell_type":"code","source":"embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:01:10.707960Z","iopub.execute_input":"2023-05-05T19:01:10.708351Z","iopub.status.idle":"2023-05-05T19:01:19.139551Z","shell.execute_reply.started":"2023-05-05T19:01:10.708316Z","shell.execute_reply":"2023-05-05T19:01:19.138399Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc436b2f3db4cecb2249e546541f4f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2e265eb90f4db991a4fe73d6b87f1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c12465bc0ee44f8698845ad7e3347a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba13169afc949fe813532fc7f18ff59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7042164d178045b48b7f35b3a18038f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba601dad7714a94ad130a67821fa5af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25d9f12080884de596ea76da555e60b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77e0fdba07394586944eca3f8aa026cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc42a18fc1b4c09ad810877aa725111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eeb25a9a731484688ec67145d7dcb84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d90f898cd54de99be25a49584c7296"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"402fddbf59944c44a8361ed85da636b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d294817cc70348ee89053cc087cc0ed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e36323a74547b68c400dad6f63450d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430d53f9ac3e407b90701bef899dbb1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f30df2c9593d41dabe697df8c783035d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e70073b6ed684a8a947e4c0ed8d196a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb30060879442a3b42a4219222fd2a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf51cf59696a4f05b041cf9424e31c6d"}},"metadata":{}}]},{"cell_type":"code","source":"def get_embeddings(data):\n    return embedder.encode(data, convert_to_tensor=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:01:34.704508Z","iopub.execute_input":"2023-05-05T19:01:34.704878Z","iopub.status.idle":"2023-05-05T19:01:34.710327Z","shell.execute_reply.started":"2023-05-05T19:01:34.704845Z","shell.execute_reply":"2023-05-05T19:01:34.709166Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"abstract_embeddings = get_embeddings(cleaned_docs)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:01:46.334776Z","iopub.execute_input":"2023-05-05T19:01:46.335399Z","iopub.status.idle":"2023-05-05T19:13:13.331596Z","shell.execute_reply.started":"2023-05-05T19:01:46.335360Z","shell.execute_reply":"2023-05-05T19:13:13.330532Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/15126 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc9e8e246eaa4165b020628787dc959a"}},"metadata":{}}]},{"cell_type":"code","source":"query = ['temporal expression extraction']\nquery_embedding = get_embeddings(query)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:27:25.137991Z","iopub.execute_input":"2023-05-05T19:27:25.138741Z","iopub.status.idle":"2023-05-05T19:27:25.176799Z","shell.execute_reply.started":"2023-05-05T19:27:25.138696Z","shell.execute_reply":"2023-05-05T19:27:25.175748Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f86f1aec2b93453ca8ef6747e00e67a4"}},"metadata":{}}]},{"cell_type":"markdown","source":"Getting the top 100 hits for reranking","metadata":{}},{"cell_type":"code","source":"hits = util.semantic_search(query_embedding, abstract_embeddings, top_k=100)\nhits = hits[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:14:49.984975Z","iopub.execute_input":"2023-05-05T19:14:49.985751Z","iopub.status.idle":"2023-05-05T19:14:49.998759Z","shell.execute_reply.started":"2023-05-05T19:14:49.985703Z","shell.execute_reply":"2023-05-05T19:14:49.997631Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"hits[:5]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:14:58.976907Z","iopub.execute_input":"2023-05-05T19:14:58.977306Z","iopub.status.idle":"2023-05-05T19:14:58.984929Z","shell.execute_reply.started":"2023-05-05T19:14:58.977268Z","shell.execute_reply":"2023-05-05T19:14:58.983827Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[{'corpus_id': 16069, 'score': 0.7779459953308105},\n {'corpus_id': 154036, 'score': 0.6949880719184875},\n {'corpus_id': 66896, 'score': 0.6799792051315308},\n {'corpus_id': 13600, 'score': 0.6713610291481018},\n {'corpus_id': 192765, 'score': 0.6707550287246704}]"},"metadata":{}}]},{"cell_type":"markdown","source":"Re-ranking all the hits using cross encoder","metadata":{}},{"cell_type":"markdown","source":"Preparing cross encoder input","metadata":{}},{"cell_type":"code","source":"cross_inp = [[query[0], cleaned_docs[hit['corpus_id']]] for hit in hits]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:20:42.056497Z","iopub.execute_input":"2023-05-05T19:20:42.056873Z","iopub.status.idle":"2023-05-05T19:20:42.062453Z","shell.execute_reply.started":"2023-05-05T19:20:42.056841Z","shell.execute_reply":"2023-05-05T19:20:42.061364Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"cross_inp[:5]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:20:42.500333Z","iopub.execute_input":"2023-05-05T19:20:42.500701Z","iopub.status.idle":"2023-05-05T19:20:42.508176Z","shell.execute_reply.started":"2023-05-05T19:20:42.500668Z","shell.execute_reply":"2023-05-05T19:20:42.507020Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[['temporal expression extraction',\n  'temporal expression normalisation in natural language texts   automatic annotation of temporal expressions is a research challenge of great\\ninterest in the field of information extraction  in this report  i describe a\\nnovel rule based architecture  built on top of a pre existing system  which is\\nable to normalise temporal expressions detected in english texts  gold standard\\ntemporally annotated resources are limited in size and this makes research\\ndifficult  the proposed system outperforms the state of the art systems with\\nrespect to tempeval 2 shared task  value attribute  and achieves substantially\\nbetter results with respect to the pre existing system on top of which it has\\nbeen developed  i will also introduce a new free corpus consisting of 2822\\nunique annotated temporal expressions  both the corpus and the system are\\nfreely available on line \\n'],\n ['temporal expression extraction',\n  'temporal information extraction by predicting relative time lines   the current leading paradigm for temporal information extraction from text\\nconsists of three phases   1  recognition of events and temporal expressions \\n 2  recognition of temporal relations among them  and  3  time line\\nconstruction from the temporal relations  in contrast to the first two phases \\nthe last phase  time line construction  received little attention and is the\\nfocus of this work  in this paper  we propose a new method to construct a\\nlinear time line from a set of  extracted  temporal relations  but more\\nimportantly  we propose a novel paradigm in which we directly predict start and\\nend points for events from the text  constituting a time line without going\\nthrough the intermediate step of prediction of temporal relations as in earlier\\nwork  within this paradigm  we propose two models that predict in linear\\ncomplexity  and a new training loss using timeml style annotations  yielding\\npromising results \\n'],\n ['temporal expression extraction',\n  'on timeml compliant temporal expression extraction in turkish   it is commonly acknowledged that temporal expression extractors are important\\ncomponents of larger natural language processing systems like information\\nretrieval and question answering systems  extraction and normalization of\\ntemporal expressions in turkish has not been given attention so far except the\\nextraction of some date and time expressions within the course of named entity\\nrecognition  as timeml is the current standard of temporal expression and event\\nannotation in natural language texts  in this paper  we present an analysis of\\ntemporal expressions in turkish based on the related timeml classification\\n i e   date  time  duration  and set expressions   we have created a lexicon\\nfor turkish temporal expressions and devised considerably wide coverage\\npatterns using the lexical classes as the building blocks  we believe that the\\nproposed patterns  together with convenient normalization rules  can be readily\\nused by prospective temporal expression extraction tools for turkish \\n'],\n ['temporal expression extraction',\n  'massively increasing timex3 resources  a transduction approach   automatic annotation of temporal expressions is a research challenge of great\\ninterest in the field of information extraction  gold standard\\ntemporally annotated resources are limited in size  which makes research using\\nthem difficult  standards have also evolved over the past decade  so not all\\ntemporally annotated data is in the same format  we vastly increase available\\nhuman annotated temporal expression resources by converting older format\\nresources to timeml timex3  this task is difficult due to differing annotation\\nmethods  we present a robust conversion tool and a new  large temporal\\nexpression resource  using this  we evaluate our conversion process by using it\\nas training data for an existing timeml annotation tool  achieving a 0 87 f1\\nmeasure    better than any system in the tempeval 2 timex recognition exercise \\n'],\n ['temporal expression extraction',\n  'cogcomptime  a tool for understanding time in natural language text   automatic extraction of temporal information in text is an important\\ncomponent of natural language understanding  it involves two basic tasks   1 \\nunderstanding time expressions that are mentioned explicitly in text  e g  \\nfebruary 27  1998 or tomorrow   and  2  understanding temporal information that\\nis conveyed implicitly via relations  in this paper  we introduce cogcomptime \\na system that has these two important functionalities  it incorporates the most\\nrecent progress  achieves state of the art performance  and is publicly\\navailable 1 we believe that this demo will be useful for multiple time aware\\napplications and provide valuable insight for future research in temporal\\nunderstanding \\n']]"},"metadata":{}}]},{"cell_type":"code","source":"cross_scores = cross_encoder.predict(cross_inp)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:20:46.622318Z","iopub.execute_input":"2023-05-05T19:20:46.623431Z","iopub.status.idle":"2023-05-05T19:20:46.896252Z","shell.execute_reply.started":"2023-05-05T19:20:46.623384Z","shell.execute_reply":"2023-05-05T19:20:46.895293Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f8adc2b37d4b7fb8a7910ab681d239"}},"metadata":{}}]},{"cell_type":"code","source":"cross_scores","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:21:08.138600Z","iopub.execute_input":"2023-05-05T19:21:08.139182Z","iopub.status.idle":"2023-05-05T19:21:08.147032Z","shell.execute_reply.started":"2023-05-05T19:21:08.139143Z","shell.execute_reply":"2023-05-05T19:21:08.145870Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"array([  7.040183  ,   6.9775553 ,   8.60662   ,   6.6921773 ,\n         4.8595524 ,   4.453336  ,   0.23782401,   5.2521605 ,\n        -0.03346934,   5.7105794 ,  -2.97583   ,  -5.008688  ,\n         4.5531425 ,  -1.3967086 ,  -0.59001285,   5.164037  ,\n         1.280134  ,   4.7221255 ,   3.7857106 ,   6.349493  ,\n        -0.31447238,   4.8841915 ,   1.5201616 ,   3.6251032 ,\n         1.9316106 ,   2.231379  ,  -4.74449   ,   3.029835  ,\n        -4.7363033 ,   2.6730936 ,   3.7265553 ,  -1.1291184 ,\n        -5.1323185 ,   0.6796724 ,   2.254343  ,   1.5796994 ,\n         4.3050575 ,  -2.2977514 ,  -2.5092812 ,  -1.3380922 ,\n         3.170241  ,  -1.7978094 ,   1.0735011 ,  -0.3843995 ,\n        -4.662538  ,  -5.0860114 ,   4.263301  ,   3.6782656 ,\n         2.560791  ,   5.199521  ,  -0.58925045,  -6.0470433 ,\n        -9.1827545 ,  -1.4201727 ,   1.0260693 ,   2.036706  ,\n        -1.8110634 ,  -4.968667  ,   6.988722  ,  -4.896039  ,\n        -4.125615  ,   4.179108  ,   3.663168  ,  -2.4860525 ,\n        -1.8722426 ,  -5.25442   ,   5.3639183 ,  -4.8314104 ,\n        -7.914457  , -10.327501  ,  -0.25227377,   2.8091369 ,\n         4.8062477 ,   0.22217369,  -0.104516  , -11.425529  ,\n        -5.518881  ,  -4.794613  ,  -0.7812918 ,  -6.255868  ,\n        -5.8792405 ,  -6.374653  ,  -4.705637  ,  -8.295058  ,\n         3.4280493 ,  -9.001131  ,  -3.6406307 ,   5.416475  ,\n        -4.908823  ,  -7.3702826 ,  -2.5072155 ,   2.119667  ,\n       -11.272233  ,   3.5566802 ,  -3.8922517 ,  -3.562872  ,\n        -5.6625915 ,  -3.665423  , -11.31393   ,   6.0343156 ],\n      dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"for idx in range(len(cross_scores)):\n    hits[idx]['cross-score'] = cross_scores[idx]\n    \ncross_score_sorted_hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\ncross_score_sorted_hits[:5]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:24:06.498324Z","iopub.execute_input":"2023-05-05T19:24:06.499017Z","iopub.status.idle":"2023-05-05T19:24:06.509053Z","shell.execute_reply.started":"2023-05-05T19:24:06.498980Z","shell.execute_reply":"2023-05-05T19:24:06.507845Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"[{'corpus_id': 66896, 'score': 0.6799792051315308, 'cross-score': 8.60662},\n {'corpus_id': 16069, 'score': 0.7779459953308105, 'cross-score': 7.040183},\n {'corpus_id': 400663, 'score': 0.5173180103302002, 'cross-score': 6.988722},\n {'corpus_id': 154036, 'score': 0.6949880719184875, 'cross-score': 6.9775553},\n {'corpus_id': 13600, 'score': 0.6713610291481018, 'cross-score': 6.6921773}]"},"metadata":{}}]},{"cell_type":"code","source":"hits[:5]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:24:18.896718Z","iopub.execute_input":"2023-05-05T19:24:18.897100Z","iopub.status.idle":"2023-05-05T19:24:18.905664Z","shell.execute_reply.started":"2023-05-05T19:24:18.897066Z","shell.execute_reply":"2023-05-05T19:24:18.904415Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[{'corpus_id': 16069, 'score': 0.7779459953308105, 'cross-score': 7.040183},\n {'corpus_id': 154036, 'score': 0.6949880719184875, 'cross-score': 6.9775553},\n {'corpus_id': 66896, 'score': 0.6799792051315308, 'cross-score': 8.60662},\n {'corpus_id': 13600, 'score': 0.6713610291481018, 'cross-score': 6.6921773},\n {'corpus_id': 192765, 'score': 0.6707550287246704, 'cross-score': 4.8595524}]"},"metadata":{}}]},{"cell_type":"markdown","source":"If you see from the result, the bi-encoder was also able to do fairly well as 4 out of top 5 results overlap between the bi-encoder and cross-encoder output.","metadata":{}},{"cell_type":"markdown","source":"Let's write a method combining all the previous steps to perform search for our query on the abstract corpus.","metadata":{}},{"cell_type":"code","source":"def get_top_hits(query):\n    query_embedding = get_embeddings([query])\n    hits = util.semantic_search(query_embedding, abstract_embeddings, top_k=100)[0]\n    \n    cross_inp = [[query, cleaned_docs[hit['corpus_id']]] for hit in hits]\n    cross_scores = cross_encoder.predict(cross_inp)\n    \n    for idx in range(len(cross_scores)):\n        hits[idx]['cross-score'] = cross_scores[idx]\n    \n    cross_score_sorted_hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n    \n    ## get the abstract and title for top 3 hits before and after re-ranking for comparison\n    for idx in range(2):\n        print('Hit {} before ranking - '.format(idx+1))\n        print('\\nScore - ', hits[idx]['score'])\n        print('\\nAbstract - ', docs_df.iloc[hits[idx]['corpus_id']].abstract)\n        print('\\nTitle - ', docs_df.iloc[hits[idx]['corpus_id']].title)\n        \n        print('\\n\\nHit {} after ranking - '.format(idx+1))\n        print('\\nCross encoder Score - ', cross_score_sorted_hits[idx]['cross-score'])\n        print('\\nAbstract - ', docs_df.iloc[cross_score_sorted_hits[idx]['corpus_id']].abstract)\n        print('\\nTitle - ', docs_df.iloc[cross_score_sorted_hits[idx]['corpus_id']].title)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:41:14.680525Z","iopub.execute_input":"2023-05-05T19:41:14.681115Z","iopub.status.idle":"2023-05-05T19:41:14.691713Z","shell.execute_reply.started":"2023-05-05T19:41:14.681077Z","shell.execute_reply":"2023-05-05T19:41:14.690581Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"get_top_hits('what is temporal expression extraction?')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:41:19.784329Z","iopub.execute_input":"2023-05-05T19:41:19.784700Z","iopub.status.idle":"2023-05-05T19:41:20.087075Z","shell.execute_reply.started":"2023-05-05T19:41:19.784666Z","shell.execute_reply":"2023-05-05T19:41:20.086017Z"},"trusted":true},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb174fe81b746469ebf304f8eeacf77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a63f5299d2784958b3ace86a93594bde"}},"metadata":{}},{"name":"stdout","text":"Hit 1 before ranking - \n\nScore -  0.6939192414283752\n\nAbstract -    Automatic annotation of temporal expressions is a research challenge of great\ninterest in the field of information extraction. In this report, I describe a\nnovel rule-based architecture, built on top of a pre-existing system, which is\nable to normalise temporal expressions detected in English texts. Gold standard\ntemporally-annotated resources are limited in size and this makes research\ndifficult. The proposed system outperforms the state-of-the-art systems with\nrespect to TempEval-2 Shared Task (value attribute) and achieves substantially\nbetter results with respect to the pre-existing system on top of which it has\nbeen developed. I will also introduce a new free corpus consisting of 2822\nunique annotated temporal expressions. Both the corpus and the system are\nfreely available on-line.\n\n\nTitle -  Temporal expression normalisation in natural language texts\n\n\nHit 1 after ranking - \n\nCross encoder Score -  7.6009026\n\nAbstract -    It is commonly acknowledged that temporal expression extractors are important\ncomponents of larger natural language processing systems like information\nretrieval and question answering systems. Extraction and normalization of\ntemporal expressions in Turkish has not been given attention so far except the\nextraction of some date and time expressions within the course of named entity\nrecognition. As TimeML is the current standard of temporal expression and event\nannotation in natural language texts, in this paper, we present an analysis of\ntemporal expressions in Turkish based on the related TimeML classification\n(i.e., date, time, duration, and set expressions). We have created a lexicon\nfor Turkish temporal expressions and devised considerably wide-coverage\npatterns using the lexical classes as the building blocks. We believe that the\nproposed patterns, together with convenient normalization rules, can be readily\nused by prospective temporal expression extraction tools for Turkish.\n\n\nTitle -  On TimeML-Compliant Temporal Expression Extraction in Turkish\nHit 2 before ranking - \n\nScore -  0.6514966487884521\n\nAbstract -    Automatic extraction of temporal information in text is an important\ncomponent of natural language understanding. It involves two basic tasks: (1)\nUnderstanding time expressions that are mentioned explicitly in text (e.g.,\nFebruary 27, 1998 or tomorrow), and (2) Understanding temporal information that\nis conveyed implicitly via relations. In this paper, we introduce CogCompTime,\na system that has these two important functionalities. It incorporates the most\nrecent progress, achieves state-of-the-art performance, and is publicly\navailable.1 We believe that this demo will be useful for multiple time-aware\napplications and provide valuable insight for future research in temporal\nunderstanding.\n\n\nTitle -  CogCompTime: A Tool for Understanding Time in Natural Language Text\n\n\nHit 2 after ranking - \n\nCross encoder Score -  6.5255384\n\nAbstract -    Automatic annotation of temporal expressions is a research challenge of great\ninterest in the field of information extraction. In this report, I describe a\nnovel rule-based architecture, built on top of a pre-existing system, which is\nable to normalise temporal expressions detected in English texts. Gold standard\ntemporally-annotated resources are limited in size and this makes research\ndifficult. The proposed system outperforms the state-of-the-art systems with\nrespect to TempEval-2 Shared Task (value attribute) and achieves substantially\nbetter results with respect to the pre-existing system on top of which it has\nbeen developed. I will also introduce a new free corpus consisting of 2822\nunique annotated temporal expressions. Both the corpus and the system are\nfreely available on-line.\n\n\nTitle -  Temporal expression normalisation in natural language texts\n","output_type":"stream"}]},{"cell_type":"code","source":"get_top_hits('what is cross entropy loss?')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:41:42.422530Z","iopub.execute_input":"2023-05-05T19:41:42.422898Z","iopub.status.idle":"2023-05-05T19:41:42.748936Z","shell.execute_reply.started":"2023-05-05T19:41:42.422866Z","shell.execute_reply":"2023-05-05T19:41:42.747874Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad16b4d152b4b40810b08f207d4162d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12fe22ce0e04861b1b2c66567e7d98a"}},"metadata":{}},{"name":"stdout","text":"Hit 1 before ranking - \n\nScore -  0.5716948509216309\n\nAbstract -    Minimizing cross-entropy is a widely used method for training artificial\nneural networks. Many training procedures based on backpropagation use\ncross-entropy directly as their loss function. Instead, this theoretical essay\ninvestigates a dual process model with two processes, in which one process\nminimizes the Kullback-Leibler divergence while its dual counterpart minimizes\nthe Shannon entropy. Postulating that learning consists of two dual processes\ncomplementing each other, the model defines an equilibrium state for both\nprocesses in which the loss function assumes its minimum. An advantage of the\nproposed model is that it allows deriving the optimal learning rate and\nmomentum weight to update network weights for backpropagation. Furthermore, the\nmodel introduces the golden ratio and complex numbers as important new concepts\nin machine learning.\n\n\nTitle -  A Dual Process Model for Optimizing Cross Entropy in Neural Networks\n\n\nHit 1 after ranking - \n\nCross encoder Score -  8.162387\n\nAbstract -    Cross-entropy loss is the standard metric used to train classification models\nin deep learning and gradient boosting. It is well-known that this loss\nfunction fails to account for similarities between the different values of the\ntarget. We propose a generalization of entropy called {\\em structured entropy}\nwhich uses a random partition to incorporate the structure of the target\nvariable in a manner which retains many theoretical properties of standard\nentropy. We show that a structured cross-entropy loss yields better results on\nseveral classification problems where the target variable has an a priori known\nstructure. The approach is simple, flexible, easily computable, and does not\nrely on a hierarchically defined notion of structure.\n\n\nTitle -  Loss Functions for Classification using Structured Entropy\nHit 2 before ranking - \n\nScore -  0.5693882703781128\n\nAbstract -    The R\\'{e}nyi cross-entropy measure between two distributions, a\ngeneralization of the Shannon cross-entropy, was recently used as a loss\nfunction for the improved design of deep learning generative adversarial\nnetworks. In this work, we examine the properties of this measure and derive\nclosed-form expressions for it when one of the distributions is fixed and when\nboth distributions belong to the exponential family. We also analytically\ndetermine a formula for the cross-entropy rate for stationary Gaussian\nprocesses and for finite-alphabet Markov sources.\n\n\nTitle -  On the R\\'{e}nyi Cross-Entropy\n\n\nHit 2 after ranking - \n\nCross encoder Score -  7.612485\n\nAbstract -    Cross-entropy is a widely used loss function in applications. It coincides\nwith the logistic loss applied to the outputs of a neural network, when the\nsoftmax is used. But, what guarantees can we rely on when using cross-entropy\nas a surrogate loss? We present a theoretical analysis of a broad family of\nlosses, comp-sum losses, that includes cross-entropy (or logistic loss),\ngeneralized cross-entropy, the mean absolute error and other loss\ncross-entropy-like functions. We give the first $H$-consistency bounds for\nthese loss functions. These are non-asymptotic guarantees that upper bound the\nzero-one loss estimation error in terms of the estimation error of a surrogate\nloss, for the specific hypothesis set $H$ used. We further show that our bounds\nare tight. These bounds depend on quantities called minimizability gaps, which\nonly depend on the loss function and the hypothesis set. To make them more\nexplicit, we give a specific analysis of these gaps for comp-sum losses. We\nalso introduce a new family of loss functions, smooth adversarial comp-sum\nlosses, derived from their comp-sum counterparts by adding in a related smooth\nterm. We show that these loss functions are beneficial in the adversarial\nsetting by proving that they admit $H$-consistency bounds. This leads to new\nadversarial robustness algorithms that consist of minimizing a regularized\nsmooth adversarial comp-sum loss. While our main purpose is a theoretical\nanalysis, we also present an extensive empirical analysis comparing comp-sum\nlosses. We further report the results of a series of experiments demonstrating\nthat our adversarial robustness algorithms outperform the current\nstate-of-the-art, while also achieving a superior non-adversarial accuracy.\n\n\nTitle -  Cross-Entropy Loss Functions: Theoretical Analysis and Applications\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For the first query, both the resonses are fairl even before and after ranking, we do get much better and relevant responses after reranking for the second query.","metadata":{}},{"cell_type":"code","source":"get_top_hits('rebalancing in kafka')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T19:43:22.930118Z","iopub.execute_input":"2023-05-05T19:43:22.930745Z","iopub.status.idle":"2023-05-05T19:43:23.291891Z","shell.execute_reply.started":"2023-05-05T19:43:22.930686Z","shell.execute_reply":"2023-05-05T19:43:23.290779Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bace8dd1de564ef2b24b8de8e0182262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be14bd1e79614175bb9b380c22988654"}},"metadata":{}},{"name":"stdout","text":"Hit 1 before ranking - \n\nScore -  0.5709049701690674\n\nAbstract -    Apache Kafka addresses the general problem of delivering extreme high volume\nevent data to diverse consumers via a publish-subscribe messaging system. It\nuses partitions to scale a topic across many brokers for producers to write\ndata in parallel, and also to facilitate parallel reading of consumers. Even\nthough Apache Kafka provides some out of the box optimizations, it does not\nstrictly define how each topic shall be efficiently distributed into\npartitions. The well-formulated fine-tuning that is needed in order to improve\nan Apache Kafka cluster performance is still an open research problem. In this\npaper, we first model the Apache Kafka topic partitioning process for a given\ntopic. Then, given the set of brokers, constraints and application requirements\non throughput, OS load, replication latency and unavailability, we formulate\nthe optimization problem of finding how many partitions are needed and show\nthat it is computationally intractable, being an integer program. Furthermore,\nwe propose two simple, yet efficient heuristics to solve the problem: the first\ntries to minimize and the second to maximize the number of brokers used in the\ncluster. Finally, we evaluate its performance via large-scale simulations,\nconsidering as benchmarks some Apache Kafka cluster configuration\nrecommendations provided by Microsoft and Confluent. We demonstrate that,\nunlike the recommendations, the proposed heuristics respect the hard\nconstraints on replication latency and perform better w.r.t. unavailability\ntime and OS load, using the system resources in a more prudent way.\n\n\nTitle -  On Efficiently Partitioning a Topic in Apache Kafka\n\n\nHit 1 after ranking - \n\nCross encoder Score -  4.374723\n\nAbstract -    Message brokers enable asynchronous communication between data producers and\nconsumers in distributed environments by assigning messages to ordered queues.\nMessage broker systems often provide with mechanisms to parallelize tasks\nbetween consumers to increase the rate at which data is consumed. The\nconsumption rate must exceed the production rate or queues would grow\nindefinitely. Still, consumers are costly and their number should be minimized.\nWe model the problem of determining the required number of consumers, and the\npartition-consumer assignments, as a variable item size bin packing variant.\nData cannot be read when a queue is being migrated to another consumer. Hence,\nwe propose the R-score metric to account for these rebalancing costs. Then, we\nintroduce an assortment of R-score based algorithms, and compare their\nperformance to established heuristics for the Bin Packing Problem for this\napplication. We instantiate our method within an existing system, demonstrating\nits effectiveness. Our approach guarantees adequate consumption rates something\nthe previous system was unable to at lower operational costs.\n\n\nTitle -  Kafka Consumer Group Autoscaler\nHit 2 before ranking - \n\nScore -  0.5580893158912659\n\nAbstract -    Message brokers enable asynchronous communication between data producers and\nconsumers in distributed environments by assigning messages to ordered queues.\nMessage broker systems often provide with mechanisms to parallelize tasks\nbetween consumers to increase the rate at which data is consumed. The\nconsumption rate must exceed the production rate or queues would grow\nindefinitely. Still, consumers are costly and their number should be minimized.\nWe model the problem of determining the required number of consumers, and the\npartition-consumer assignments, as a variable item size bin packing variant.\nData cannot be read when a queue is being migrated to another consumer. Hence,\nwe propose the R-score metric to account for these rebalancing costs. Then, we\nintroduce an assortment of R-score based algorithms, and compare their\nperformance to established heuristics for the Bin Packing Problem for this\napplication. We instantiate our method within an existing system, demonstrating\nits effectiveness. Our approach guarantees adequate consumption rates something\nthe previous system was unable to at lower operational costs.\n\n\nTitle -  Kafka Consumer Group Autoscaler\n\n\nHit 2 after ranking - \n\nCross encoder Score -  0.54502034\n\nAbstract -    Apache Kafka addresses the general problem of delivering extreme high volume\nevent data to diverse consumers via a publish-subscribe messaging system. It\nuses partitions to scale a topic across many brokers for producers to write\ndata in parallel, and also to facilitate parallel reading of consumers. Even\nthough Apache Kafka provides some out of the box optimizations, it does not\nstrictly define how each topic shall be efficiently distributed into\npartitions. The well-formulated fine-tuning that is needed in order to improve\nan Apache Kafka cluster performance is still an open research problem. In this\npaper, we first model the Apache Kafka topic partitioning process for a given\ntopic. Then, given the set of brokers, constraints and application requirements\non throughput, OS load, replication latency and unavailability, we formulate\nthe optimization problem of finding how many partitions are needed and show\nthat it is computationally intractable, being an integer program. Furthermore,\nwe propose two simple, yet efficient heuristics to solve the problem: the first\ntries to minimize and the second to maximize the number of brokers used in the\ncluster. Finally, we evaluate its performance via large-scale simulations,\nconsidering as benchmarks some Apache Kafka cluster configuration\nrecommendations provided by Microsoft and Confluent. We demonstrate that,\nunlike the recommendations, the proposed heuristics respect the hard\nconstraints on replication latency and perform better w.r.t. unavailability\ntime and OS load, using the system resources in a more prudent way.\n\n\nTitle -  On Efficiently Partitioning a Topic in Apache Kafka\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We see for the above query, both the systems has returned the same abstract but in different order. Ranking the hits have actually helped the more relevant abstract to be ranked above the second abstract. Whereas before ranking, the more relevant abstract is second in the order.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}